from flask import Flask, request, jsonify, render_template, send_file
import re
import io
from fpdf import FPDF
from datetime import datetime, timedelta
from flask_socketio import SocketIO
import os
import json
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
import threading
import queue
import time
from pymongo import MongoClient
from werkzeug.utils import secure_filename
import zipfile
import PyPDF2
import fitz  # PyMuPDF
import xml.etree.ElementTree as ET
from androguard.core.bytecodes.apk import APK
from androguard.core.bytecodes.dvm import DalvikVMFormat
from androguard.core.analysis.analysis import Analysis
from features_extract import (
    extract_url_features,
    extract_keyword_features,
    extract_content_features,
    extract_domain_features,
    extract_redirection_count,
    get_certificate_info,
    get_domain_age,
    get_dns_record_count,
    check_spf_dmarc,
    is_shortened_url,
    analyze_url,
    calculate_risk_score,
    check_url_virustotal,
    check_google_safe_browsing
)
import concurrent.futures
from functools import partial, lru_cache
from urllib.parse import urlparse
import sqlite3
from contextlib import contextmanager


app = Flask(__name__, static_folder='static', template_folder='templates')
socketio = SocketIO(app, cors_allowed_origins="*")

@contextmanager
def get_db_connection():
    try:
        client = MongoClient('mongodb://localhost:27018/')
        db = client['phishing_detection']
        yield db
    finally:
        client.close()

# MongoDB connection
try:
    client = MongoClient('mongodb://localhost:27018/')
    db = client['phishing_detection']
except Exception as e:
    print(f"Error connecting to MongoDB: {e}")
    db = None

# Mock database for testing
mock_db = [
    {
        "url": "https://example-bank.com",
        "title": "Example Bank",
        "is_phishing": False,
        "timestamp": (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S'),
        "risk_score": 15
    },
    {
        "url": "https://suspicious-bank-login.com",
        "title": "Bank Login",
        "is_phishing": True,
        "timestamp": (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d %H:%M:%S'),
        "risk_score": 85
    },
    {
        "url": "https://secure-banking.com",
        "title": "Secure Banking",
        "is_phishing": False,
        "timestamp": (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d %H:%M:%S'),
        "risk_score": 20
    },
    {
        "url": "https://bank-verify-account.com",
        "title": "Verify Your Account",
        "is_phishing": True,
        "timestamp": (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d %H:%M:%S'),
        "risk_score": 75
    },
    {
        "url": "https://legitimate-bank.com",
        "title": "Legitimate Bank",
        "is_phishing": False,
        "timestamp": (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d %H:%M:%S'),
        "risk_score": 10
    }
]

# Queue for storing crawling results
crawl_results = queue.Queue()

# Home route
@app.route('/')
def home():
    return render_template('index.html')

# Feature Extraction API
@app.route('/index', methods=['POST'])
def index():
    try:
        data = request.get_json()
        url = data.get("url")

        if not url:
            return jsonify({"error": "URL is missing."}), 400

        # Extract features in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # Submit all feature extraction tasks
            futures = {
                'url_features': executor.submit(extract_url_features, url),
                'keyword_features': executor.submit(extract_keyword_features, url),
                'content_features': executor.submit(extract_content_features, url),
                'domain_features': executor.submit(extract_domain_features, url),
                'redirection_count': executor.submit(extract_redirection_count, url),
                'certificate_info': executor.submit(get_certificate_info, url),
                'domain_age': executor.submit(get_domain_age, url),
                'dns_record_count': executor.submit(get_dns_record_count, url),
                'check_spf_dmarc': executor.submit(check_spf_dmarc, url),
                'is_shortened_url': executor.submit(is_shortened_url, url),
                'virus_total': executor.submit(check_url_virustotal, url),
                'google_safe_browsing': executor.submit(check_google_safe_browsing, url)
            }
            
            # Collect results as they complete
            features = {}
            for key, future in futures.items():
                try:
                    features[key] = future.result(timeout=10)  # 10 second timeout per feature
                except Exception as e:
                    print(f"Error extracting {key}: {str(e)}")
                    features[key] = None

        # Calculate risk score
        risk_score = calculate_risk_score(analyze_url(url))
        is_phishing = risk_score >= 40

        # Store in mock database with timestamp
        scan_result = {
            "url": url,
            "title": url,
            "is_phishing": is_phishing,
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "risk_score": risk_score
        }
        mock_db.append(scan_result)

        # If MongoDB is available, store there too
        if db is not None:
            try:
                db.bank_websites.insert_one({
                    "url": url,
                    "title": url,
                    "is_phishing": "1" if is_phishing else "0",
                    "crawled_at": datetime.now(),
                    "confidence_score": risk_score
                })
            except Exception as e:
                print(f"Error storing in MongoDB: {e}")

        return jsonify({
            "url": url,
            "features": features,
            "risk_score": risk_score,
            "verdict": (
                "Highly suspicious ⚠" if risk_score >= 70 else
                "Moderately suspicious ⚠" if risk_score >= 40 else
                "Likely safe ✅ (Still verify manually)"
            )
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Get Crawled Data API
@app.route('/get_data', methods=['GET'])
def get_data():
    try:
        # Return last 50 entries from mock database
        return jsonify({
            "message": "Cleaned crawled data retrieved successfully",
            "data": mock_db[-50:] if mock_db else []
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Generate and Download PDF Report API
@app.route('/download_report', methods=['POST'])
def download_report():
    try:
        data = request.get_json()
        url = data.get("url")
        if not url:
            return jsonify({"error": "URL is missing"}), 400

        # Extract features and calculate risk score
        features = analyze_url(url)
        risk_score = calculate_risk_score(features)

        # Create PDF
        class PDF(FPDF):
            def header(self):
                # Set black background for the entire page
                self.set_fill_color(0, 0, 0)
                self.rect(0, 0, self.w, self.h, 'F')
                
                # Add logo
                try:
                    self.image('static/images/logo.png', 10, 8, 33)
                except:
                    pass  # Continue if logo not found
                
                # Add decorative line
                self.set_draw_color(255, 0, 0)
                self.set_line_width(0.5)
                self.line(10, 45, self.w - 10, 45)
                
                self.set_text_color(255, 255, 255)  # White color for header
                self.set_font('Arial', 'B', 24)
                self.cell(0, 20, 'Website Security Report', 0, 1, 'C')
                
                # Add timestamp
                self.set_font('Arial', 'I', 10)
                self.cell(0, 10, f'Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}', 0, 1, 'C')
                self.ln(10)

            def footer(self):
                self.set_y(-15)
                # Add decorative line
                self.set_draw_color(255, 0, 0)
                self.set_line_width(0.5)
                self.line(10, self.h - 20, self.w - 10, self.h - 20)
                
                self.set_text_color(255, 255, 255)  # White color for footer
                self.set_font('Arial', 'I', 10)
                self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')

            def create_table(self, headers, data, col_widths):
                # Table header
                self.set_fill_color(40, 40, 40)
                self.set_text_color(255, 255, 255)  # White color for headers
                self.set_font('Arial', 'B', 12)
                self.set_draw_color(255, 0, 0)  # Red border
                self.set_line_width(0.3)
                
                # Draw header cells with borders
                for i, header in enumerate(headers):
                    self.cell(col_widths[i], 10, header, 1, 0, 'C', True)
                self.ln()

                # Table data
                self.set_text_color(255, 0, 0)  # Red color for data
                self.set_font('Arial', '', 11)
                for row in data:
                    for i, value in enumerate(row):
                        self.cell(col_widths[i], 10, str(value), 1, 0, 'L')
                    self.ln()

            def section_title(self, title):
                self.ln(5)
                self.set_text_color(255, 255, 255)  # White color
                self.set_font('Arial', 'B', 14)
                self.cell(0, 10, title, 0, 1, 'L')
                # Add decorative line under section title
                self.set_draw_color(255, 0, 0)
                self.set_line_width(0.3)
                self.line(self.get_x(), self.get_y(), self.w - 10, self.get_y())
                self.ln(5)

        # Initialize PDF
        pdf = PDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_fill_color(0, 0, 0)  # Black background

        # URL Section
        pdf.section_title('Analyzed URL')
        pdf.set_text_color(255, 0, 0)  # Red color for URL
        pdf.set_font('Arial', '', 12)
        pdf.cell(0, 10, str(url), 0, 1)
        pdf.ln(5)

        # Risk Score Section
        pdf.section_title('Risk Analysis')

        # Risk Level Box
        pdf.set_draw_color(255, 0, 0)
        pdf.set_line_width(0.5)
        pdf.set_fill_color(20, 20, 20)
        pdf.rect(10, pdf.get_y(), pdf.w - 20, 30, 'FD')
        
        if risk_score >= 70:
            risk_color = (255, 0, 0)  # Red
            risk_text = "High Risk"
        elif risk_score >= 40:
            risk_color = (255, 165, 0)  # Orange
            risk_text = "Suspicious"
        else:
            risk_color = (0, 255, 0)  # Green
            risk_text = "Safe"

        pdf.set_text_color(*risk_color)
        pdf.set_font('Arial', 'B', 14)
        pdf.set_xy(15, pdf.get_y() + 5)
        pdf.cell(0, 10, f'Risk Score: {risk_score}%', 0, 1)
        pdf.set_xy(15, pdf.get_y())
        pdf.cell(0, 10, f'Risk Level: {risk_text}', 0, 1)
        pdf.ln(10)

        # URL Features Table
        pdf.section_title('URL Analysis')
        url_headers = ['Feature', 'Value']
        url_data = [
            ['URL Length', features['url']['length']],
            ['Number of Dots', features['url']['num_dots']],
            ['Number of Slashes', features['url']['num_slashes']],
            ['Number of Subdomains', features['url']['num_subdomains']],
            ['Contains IP', 'Yes' if features['url']['has_ip'] else 'No'],
            ['Uses HTTP', 'Yes' if features['url']['has_http'] else 'No'],
            ['Uses HTTPS', 'Yes' if features['url']['has_https'] else 'No'],
            ['Contains @ Symbol', 'Yes' if features['url']['has_at'] else 'No'],
            ['TLD', features['url']['tld']]
        ]
        pdf.create_table(url_headers, url_data, [80, 110])

        # Domain Information Table
        pdf.section_title('Domain Information')
        domain_headers = ['Feature', 'Value']
        domain_data = [
            ['Domain Length', features['domain']['domain_length']],
            ['Number of Subdomains', features['domain']['num_subdomains']],
            ['Contains Hyphen', 'Yes' if features['domain']['has_hyphen'] else 'No'],
            ['Suspicious TLD', 'Yes' if features['domain']['suspicious_tld'] else 'No']
        ]
        pdf.create_table(domain_headers, domain_data, [80, 110])

        # SSL Certificate Table
        pdf.section_title('SSL Certificate Information')
        ssl_headers = ['Feature', 'Value']
        ssl_data = [
            ['Certificate Issuer', features['certificate']['cert_issuer']],
            ['Certificate Validity (Days)', features['certificate']['cert_validity_days']],
            ['Days Until Expiry', features['certificate']['days_to_expiry']],
            ['Self-Signed', 'Yes' if features['certificate']['is_self_signed'] else 'No']
        ]
        pdf.create_table(ssl_headers, ssl_data, [80, 110])

        # Security Features Table
        pdf.section_title('Security Features')
        security_headers = ['Feature', 'Value']
        security_data = [
            ['SPF Present', 'Yes' if features['email']['spf_present'] else 'No'],
            ['DMARC Present', 'Yes' if features['email']['dmarc_present'] else 'No'],
            ['DNS Records Count', features['dns']['dns_record_count']],
            ['Domain Age (Days)', features['domain_age']['domain_age_days'] if features['domain_age']['domain_age_days'] != -1 else 'Unknown'],
            ['Is Shortened URL', 'Yes' if features['shortener']['is_shortened'] else 'No']
        ]
        pdf.create_table(security_headers, security_data, [80, 110])

        # VirusTotal Results Table
        pdf.section_title('VirusTotal Analysis')
        vt_headers = ['Category', 'Count']
        vt_data = [
            ['Malicious', features['virus_total'].get('malicious', 0)],
            ['Suspicious', features['virus_total'].get('suspicious', 0)],
            ['Harmless', features['virus_total'].get('harmless', 0)],
            ['Undetected', features['virus_total'].get('undetected', 0)],
            ['Timeout', features['virus_total'].get('timeout', 0)],
            ['Total Scans', features['virus_total'].get('total', 0)]
        ]
        pdf.create_table(vt_headers, vt_data, [80, 110])

        # Generate PDF
        try:
            pdf_output = pdf.output(dest='S').encode('latin-1', errors='ignore')
            buffer = io.BytesIO(pdf_output)
            buffer.seek(0)
            
            return send_file(
                buffer,
                mimetype='application/pdf',
                as_attachment=True,
                download_name='security_report.pdf'
            )
        except Exception as e:
            print(f"PDF Generation Error: {str(e)}")
            return jsonify({"error": "Failed to generate PDF report"}), 500

    except Exception as e:
        print(f"Report Generation Error: {str(e)}")
        return jsonify({"error": "Failed to process report request"}), 500

@app.route('/crawler')
def crawler():
    return render_template('crawler.html')

def check_content(url):
    try:
        return {
            'content_length': 0,  # Example result
            'has_forms': False,   # Example result
            'has_javascript': False  # Example result
        }
    except Exception as e:
        print(f"Error checking content for {url}: {str(e)}")
        return {}

def analyze_url_parallel(url):
    try:
        features = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            # Run DNS, SSL, and content checks in parallel
            dns_future = executor.submit(check_dns, url)
            ssl_future = executor.submit(check_ssl, url)
            content_future = executor.submit(check_content, url)
            
            features.update(dns_future.result())
            features.update(ssl_future.result())
            features.update(content_future.result())
        
        return features
    except Exception as e:
        print(f"Error analyzing URL {url}: {str(e)}")
        return {}

@app.route('/extract_features', methods=['POST'])
def extract_features():
    data = request.json
    url = data.get('url', '')
    title = data.get('title', '')
    content = data.get('content', '')
    is_phishing = data.get('is_phishing', '0')
    
    # Use parallel processing for URL analysis
    features = analyze_url_parallel(url)
    
    # Store the result in the queue
    crawl_results.put({
        'url': url,
        'title': title,
        'is_phishing': is_phishing,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'features': features
    })
    
    # Emit the result via WebSocket
    socketio.emit('crawl_result', {
        'url': url,
        'title': title,
        'is_phishing': is_phishing,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'features': features
    })
    
    return jsonify({'status': 'success'})

@app.route('/scan', methods=['POST'])
def scan():
    # ... existing scan code ...
    pass

@app.route('/dashboard')
def dashboard():
    try:
        if db is None:
            # If MongoDB is not available, use mock data
            total_sites = len(mock_db)
            safe_sites = sum(1 for site in mock_db if not site.get('is_phishing', False))
            phishing_sites = sum(1 for site in mock_db if site.get('is_phishing', False))
            phishing_rate = round((phishing_sites / total_sites * 100) if total_sites > 0 else 0, 2)

            # Get top phishing domains from mock data
            top_phishing_domains = [
                {
                    'url': site['url'],
                    'detection_date': site.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                    'confidence_score': site.get('risk_score', 0)
                }
                for site in mock_db if site.get('is_phishing', False)
            ][:10]

            # Get recent activity from mock data
            recent_activity = [
                {
                    'url': site['url'],
                    'timestamp': site.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                    'is_phishing': bool(site.get('is_phishing', False)),  # Convert to boolean
                    'title': site.get('title', '')
                }
                for site in mock_db[-10:]
            ]

        else:
            # Get total counts from MongoDB
            total_sites = db.bank_websites.count_documents({})
            safe_sites = db.bank_websites.count_documents({"is_phishing": "0"})
            phishing_sites = db.bank_websites.count_documents({"is_phishing": "1"})
            phishing_rate = round((phishing_sites / total_sites * 100) if total_sites > 0 else 0, 2)

            # Get top phishing domains
            top_phishing_domains = list(db.bank_websites.find(
                {"is_phishing": "1"},
                {"url": 1, "crawled_at": 1, "confidence_score": 1, "_id": 0}
            ).sort("confidence_score", -1).limit(10))

            # Get recent activity and convert is_phishing to boolean
            recent_activity = []
            for doc in db.bank_websites.find(
                {},
                {"url": 1, "crawled_at": 1, "is_phishing": 1, "title": 1, "_id": 0}
            ).sort("crawled_at", -1).limit(10):
                recent_activity.append({
                    'url': doc['url'],
                    'timestamp': doc['crawled_at'].strftime('%Y-%m-%d %H:%M:%S'),
                    'is_phishing': doc['is_phishing'] == "1",  # Convert to boolean
                    'title': doc.get('title', '')
                })

        # Get daily trends for the last 7 days
        dates = []
        safe_trend = []
        phishing_trend = []
        
        for i in range(6, -1, -1):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            dates.append(date)
            
            start_of_day = datetime.strptime(date, '%Y-%m-%d')
            end_of_day = start_of_day + timedelta(days=1)
            
            if db is None:
                # Calculate trends from mock data
                safe_count = sum(1 for site in mock_db 
                               if not site.get('is_phishing', False) 
                               and datetime.strptime(site.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')), 
                                                   '%Y-%m-%d %H:%M:%S').date() == start_of_day.date())
                phishing_count = sum(1 for site in mock_db 
                                   if site.get('is_phishing', False) 
                                   and datetime.strptime(site.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')), 
                                                       '%Y-%m-%d %H:%M:%S').date() == start_of_day.date())
            else:
                # Calculate trends from MongoDB
                safe_count = db.bank_websites.count_documents({
                    "is_phishing": "0",
                    "crawled_at": {"$gte": start_of_day, "$lt": end_of_day}
                })
                
                phishing_count = db.bank_websites.count_documents({
                    "is_phishing": "1",
                    "crawled_at": {"$gte": start_of_day, "$lt": end_of_day}
                })
            
            safe_trend.append(safe_count)
            phishing_trend.append(phishing_count)

        return render_template('dashboard.html',
                             total_sites=total_sites,
                             safe_sites=safe_sites,
                             phishing_sites=phishing_sites,
                             phishing_rate=phishing_rate,
                             top_phishing_domains=top_phishing_domains,
                             recent_activity=recent_activity,
                             dates=dates,
                             safe_trend=safe_trend,
                             phishing_trend=phishing_trend)

    except Exception as e:
        print(f"Dashboard Error: {e}")
        return render_template('dashboard.html',
                             total_sites=0,
                             safe_sites=0,
                             phishing_sites=0,
                             phishing_rate=0,
                             top_phishing_domains=[],
                             recent_activity=[],
                             dates=[],
                             safe_trend=[],
                             phishing_trend=[])

@app.route('/learn')
def learn():
    return render_template('learn.html')

@app.route('/bulk_scan', methods=['POST'])
def bulk_scan():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No data provided."}), 400

        urls = data.get("urls", [])
        if not urls:
            return jsonify({"error": "No URLs provided."}), 400

        results = []
        errors = []

        # Process URLs in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # Create a list of futures for each URL
            futures = []
            for url in urls:
                if not url.startswith(('http://', 'https://')):
                    url = f'https://{url}'
                futures.append(executor.submit(process_url, url))

            # Process results as they complete
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    if "error" in result:
                        errors.append(result)
                    else:
                        results.append(result)
                except Exception as e:
                    errors.append({"url": url, "error": str(e)})

        return jsonify({
            "results": results,
            "errors": errors,
            "total_scanned": len(urls),
            "successful_scans": len(results),
            "failed_scans": len(errors),
            "suspicious_count": sum(1 for r in results if r['risk_score'] >= 40),
            "safe_count": sum(1 for r in results if r['risk_score'] < 40)
        })

    except Exception as e:
        print(f"Bulk scan error: {str(e)}")
        return jsonify({"error": str(e)}), 500

def process_url(url):
    try:
        # Check if URL points to a file
        file_extensions = ['.pdf', '.doc', '.docx', '.txt', '.xls', '.xlsx', '.zip', '.rar']
        is_file_url = any(url.lower().endswith(ext) for ext in file_extensions)
        
        if is_file_url:
            return {
                "url": url,
                "features": {
                    "url_features": {"is_file_url": True, "file_type": url.split('.')[-1].lower()},
                    "content_features": None,
                    "domain_features": None,
                    "redirection_count": 0,
                    "certificate_info": None,
                    "domain_age": None,
                    "dns_record_count": 0,
                    "check_spf_dmarc": None,
                    "is_shortened_url": False,
                    "virus_total": None,
                    "google_safe_browsing": None
                },
                "risk_score": 30,  # Moderate risk for direct file downloads
                "verdict": "Direct file URL ⚠ (Verify source before downloading)"
            }

        # Extract features in parallel for non-file URLs
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                'url_features': executor.submit(extract_url_features, url),
                'keyword_features': executor.submit(extract_keyword_features, url),
                'content_features': executor.submit(extract_content_features, url),
                'domain_features': executor.submit(extract_domain_features, url),
                'redirection_count': executor.submit(extract_redirection_count, url),
                'certificate_info': executor.submit(get_certificate_info, url),
                'domain_age': executor.submit(get_domain_age, url),
                'dns_record_count': executor.submit(get_dns_record_count, url),
                'check_spf_dmarc': executor.submit(check_spf_dmarc, url),
                'is_shortened_url': executor.submit(is_shortened_url, url),
                'virus_total': executor.submit(check_url_virustotal, url),
                'google_safe_browsing': executor.submit(check_google_safe_browsing, url)
            }
            
            features = {}
            for key, future in futures.items():
                try:
                    features[key] = future.result(timeout=10)
                except Exception as e:
                    print(f"Error extracting {key} for {url}: {str(e)}")
                    features[key] = None

        # Calculate risk score
        risk_score = calculate_risk_score(analyze_url(url))
        is_phishing = risk_score >= 40

        # Store in mock database
        mock_db.append({
            "url": url,
            "title": url,
            "is_phishing": is_phishing,
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "risk_score": risk_score
        })

        # Store in MongoDB if available
        if db is not None:
            try:
                db.bank_websites.insert_one({
                    "url": url,
                    "title": url,
                    "is_phishing": "1" if is_phishing else "0",
                    "crawled_at": datetime.now(),
                    "confidence_score": risk_score
                })
            except Exception as e:
                print(f"Error storing in MongoDB: {e}")

        return {
            "url": url,
            "features": features,
            "risk_score": risk_score,
            "verdict": (
                "Highly suspicious ⚠" if risk_score >= 70 else
                "Moderately suspicious ⚠" if risk_score >= 40 else
                "Likely safe ✅ (Still verify manually)"
            )
        }
    except Exception as e:
        return {"url": url, "error": str(e)}

def background_processor():
    batch_size = 10
    batch = []
    
    while True:
        try:
            # Get result from queue with timeout
            result = crawl_results.get(timeout=1)
            batch.append(result)
            
            # Process batch if it reaches the size limit
            if len(batch) >= batch_size:
                process_batch(batch)
                batch = []
                
        except queue.Empty:
            # Process any remaining items in batch
            if batch:
                process_batch(batch)
                batch = []
            time.sleep(0.1)
        except Exception as e:
            print(f"Error in background processor: {str(e)}")
            time.sleep(1)

def process_batch(batch):
    try:
        with get_db_connection() as db:
            # Convert batch to MongoDB documents
            documents = [{
                'url': item['url'],
                'title': item['title'],
                'is_phishing': item['is_phishing'],
                'timestamp': item['timestamp']
            } for item in batch]
            
            # Batch insert into MongoDB
            if documents:
                db.bank_websites.insert_many(documents)
            
    except Exception as e:
        print(f"Error processing batch: {str(e)}")

# Add this to your existing configuration
UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'apk', 'pdf'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# Create uploads folder if it doesn't exist
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/apk_analyzer')
def apk_analyzer():
    return render_template('apk_analyzer.html')

@app.route('/analyze_apk', methods=['POST'])
def analyze_apk():
    print("APK analysis request received")
    if 'apk' not in request.files:
        print("No file part in request")
        return jsonify({'error': 'No file part'}), 400
    
    file = request.files['apk']
    if file.filename == '':
        print("No selected file")
        return jsonify({'error': 'No selected file'}), 400
    
    print(f"Received file: {file.filename}")
    
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        print(f"Saving file to: {filepath}")
        file.save(filepath)
        
        try:
            print("Starting APK analysis")
            # Analyze APK
            a = APK(filepath)
            print("APK loaded successfully")
            d = DalvikVMFormat(a.get_dex())
            dx = Analysis(d)
            print("DEX analysis completed")
            
            # Basic Information
            basic_info = {
                'Package Name': a.get_package(),
                'Version Name': a.get_androidversion_name(),
                'Version Code': a.get_androidversion_code(),
                'Min SDK': a.get_min_sdk_version(),
                'Target SDK': a.get_target_sdk_version(),
                'Permissions': len(a.get_permissions())
            }
            print(f"Basic info extracted: {basic_info}")
            
            # Permissions
            permissions = a.get_permissions()
            print(f"Found {len(permissions)} permissions")
            
            # Security Analysis
            security_analysis = []
            
            # Check for dangerous permissions
            dangerous_permissions = [
                'android.permission.READ_SMS',
                'android.permission.SEND_SMS',
                'android.permission.READ_CONTACTS',
                'android.permission.ACCESS_FINE_LOCATION',
                'android.permission.CAMERA',
                'android.permission.RECORD_AUDIO'
            ]
            
            for perm in dangerous_permissions:
                if perm in permissions:
                    security_analysis.append(f"Uses dangerous permission: {perm}")
            
            # Check for network security
            if 'android.permission.INTERNET' in permissions:
                security_analysis.append("App has internet access")
            
            # Check for backup and debuggable using manifest XML
            manifest = a.get_android_manifest_xml()
            if manifest is not None:
                # Register the android namespace
                ET.register_namespace('android', 'http://schemas.android.com/apk/res/android')
                
                # Check for backup
                backup_elem = manifest.find(".//application[@android:allowBackup='true']", 
                                         namespaces={'android': 'http://schemas.android.com/apk/res/android'})
                if backup_elem is not None:
                    security_analysis.append("App allows backup (potential security risk)")
                
                # Check for debuggable
                debug_elem = manifest.find(".//application[@android:debuggable='true']",
                                        namespaces={'android': 'http://schemas.android.com/apk/res/android'})
                if debug_elem is not None:
                    security_analysis.append("App is debuggable (security risk)")
            
            # Potential Risks
            risks = []
            
            # Check for suspicious activities
            suspicious_activities = [
                'android.intent.action.SEND',
                'android.intent.action.VIEW',
                'android.intent.action.EDIT'
            ]
            
            for activity in a.get_activities():
                for sus_act in suspicious_activities:
                    if sus_act in activity:
                        risks.append(f"Suspicious activity found: {activity}")
            
            # Check for native code
            if a.get_libraries():
                risks.append("App contains native code (potential security risk)")
            
            # Additional security checks
            try:
                # Check for exported activities
                exported_activities = a.get_exported_activities()
                if exported_activities:
                    security_analysis.append(f"App has {len(exported_activities)} exported activities")
                
                # Check for exported services
                exported_services = a.get_exported_services()
                if exported_services:
                    security_analysis.append(f"App has {len(exported_services)} exported services")
                
                # Check for exported receivers
                exported_receivers = a.get_exported_receivers()
                if exported_receivers:
                    security_analysis.append(f"App has {len(exported_receivers)} exported receivers")
            except:
                pass  # Skip if methods not available
            
            print("Analysis completed, cleaning up")
            # Clean up
            os.remove(filepath)
            
            result = {
                'basic_info': basic_info,
                'permissions': permissions,
                'security_analysis': security_analysis,
                'risks': risks
            }
            print(f"Returning result: {result}")
            return jsonify(result)
            
        except Exception as e:
            print(f"Error during APK analysis: {str(e)}")
            if os.path.exists(filepath):
                os.remove(filepath)
            return jsonify({'error': str(e)}), 500
    
    print("Invalid file type")
    return jsonify({'error': 'Invalid file type'}), 400
    
@app.route('/pdf_analyzer')
def pdf_analyzer():
    return render_template('pdf_analyzer.html')

@app.route('/analyze_pdf', methods=['POST'])
def analyze_pdf():
    print("PDF analysis request received")
    if 'pdf' not in request.files:
        print("No file part in request")
        return jsonify({'error': 'No file part'}), 400
    
    file = request.files['pdf']
    if file.filename == '':
        print("No selected file")
        return jsonify({'error': 'No selected file'}), 400
    
    print(f"Received file: {file.filename}")
    
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        print(f"Saving file to: {filepath}")
        file.save(filepath)
        
        try:
            print("Starting PDF analysis")
            results = {
                'basic_info': {},
                'security_analysis': [],
                'risks': [],
                'javascript': [],
                'actions': [],
                'metadata': {}
            }
            
            # Basic Information using PyPDF2
            with open(filepath, 'rb') as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                
                results['basic_info'] = {
                    'Pages': len(pdf_reader.pages),
                    'Encrypted': pdf_reader.is_encrypted,
                    'File Size': f"{os.path.getsize(filepath) / 1024:.2f} KB"
                }
                
                # Extract metadata
                if pdf_reader.metadata:
                    results['metadata'] = {
                        'Title': pdf_reader.metadata.get('/Title', 'N/A'),
                        'Author': pdf_reader.metadata.get('/Author', 'N/A'),
                        'Creator': pdf_reader.metadata.get('/Creator', 'N/A'),
                        'Producer': pdf_reader.metadata.get('/Producer', 'N/A'),
                        'Creation Date': pdf_reader.metadata.get('/CreationDate', 'N/A')
                    }
            
            # Detailed Analysis using PyMuPDF
            doc = fitz.open(filepath)
            
            # Check for JavaScript
            for page in doc:
                try:
                    # Get JavaScript actions
                    js_actions = page.get_js_actions()
                    if js_actions:
                        results['javascript'].append(f"Page {page.number + 1} contains JavaScript")
                except Exception as e:
                    print(f"Error checking JavaScript on page {page.number + 1}: {str(e)}")
            
            # Check for actions and links
            for page in doc:
                for link in page.get_links():
                    if link['kind'] == fitz.LINK_URI:
                        results['actions'].append(f"Page {page.number + 1}: External link to {link['uri']}")
                    elif link['kind'] == fitz.LINK_LAUNCH:
                        results['actions'].append(f"Page {page.number + 1}: Launch action found")
                    elif link['kind'] == fitz.LINK_GOTOR:
                        results['actions'].append(f"Page {page.number + 1}: Go-to action found")
            
            # Security Analysis
            if results['javascript']:
                results['security_analysis'].append("Document contains JavaScript (potential security risk)")
            
            if results['actions']:
                results['security_analysis'].append("Document contains interactive elements")
            
            if doc.is_encrypted:
                results['security_analysis'].append("Document is encrypted")
            
            # Check for embedded files
            for page in doc:
                for annot in page.annots():
                    if annot.type[0] == 15:  # File attachment
                        results['security_analysis'].append(f"Page {page.number + 1}: Contains embedded file")
            
            # Check for forms
            if doc.is_form_pdf:
                results['security_analysis'].append("Document contains form fields")
            
            # Potential Risks
            if len(results['actions']) > 0:
                results['risks'].append("Document contains interactive elements that could be malicious")
            
            if len(results['javascript']) > 0:
                results['risks'].append("Document contains JavaScript that could be malicious")
            
            if doc.is_encrypted:
                results['risks'].append("Encrypted document could contain hidden content")
            
            # Clean up
            doc.close()
            os.remove(filepath)
            
            print(f"Analysis completed: {results}")
            return jsonify(results)
            
        except Exception as e:
            print(f"Error during PDF analysis: {str(e)}")
            if os.path.exists(filepath):
                os.remove(filepath)
            return jsonify({'error': str(e)}), 500
    
    print("Invalid file type")
    return jsonify({'error': 'Invalid file type'}), 400

# Cache DNS results for 1 hour
@lru_cache(maxsize=1000)
def check_dns(url):
    try:
        domain = urlparse(url).netloc
        # Your existing DNS check code
        return {
            'dns_record': True,  # Example result
            'dns_age': 0  # Example result
        }
    except Exception as e:
        print(f"Error checking DNS for {url}: {str(e)}")
        return {'dns_record': False, 'dns_age': 0}

# Cache SSL results for 1 hour
@lru_cache(maxsize=1000)
def check_ssl(url):
    try:
        # Your existing SSL check code
        return {
            'ssl_verified': True,  # Example result
            'ssl_age': 0  # Example result
        }
    except Exception as e:
        print(f"Error checking SSL for {url}: {str(e)}")
        return {'ssl_verified': False, 'ssl_age': 0}

# Clear cache periodically
def clear_cache():
    while True:
        time.sleep(3600)  # Clear cache every hour
        check_dns.cache_clear()
        check_ssl.cache_clear()

# Start cache clearing thread
cache_thread = threading.Thread(target=clear_cache, daemon=True)
cache_thread.start()

if __name__ == '__main__':
    # Start background processor
    processor_thread = threading.Thread(target=background_processor, daemon=True)
    processor_thread.start()
    
    socketio.run(app, host='127.0.0.1', port=5000, debug=True)
